{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Kit - FAIR UNIVERSE: HIGGSML UNCERTAINTY CHALLENGE\n",
    "\n",
    "For Overview and Decsiptions of the competition, please visit the competition page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "`COLAB` determines whether this notebook is running on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # clone github repo\n",
    "    !git clone https://github.com/FAIR-Universe/HEP-Challenge.git\n",
    "    !pip install iminuit\n",
    "\n",
    "    # move to the HEP starting kit folder\n",
    "    %cd HEP-Challenge/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SETTINGS = {\n",
    "\"systematics\": {  # Systematics to use\n",
    "    \"tes\": True, # tau energy scale\n",
    "    \"jes\": False, # jet energy scale\n",
    "    \"soft_met\": False, # soft term in MET\n",
    "    \"ttbar_scale\": False, # ttbar scale factor\n",
    "    \"diboson_scale\": False, # diboson scale factor\n",
    "    \"bkg_scale\": False, # Background scale factor\n",
    "    },\n",
    "\"num_pseudo_experiments\" : 2 , # Number of pseudo-experiments to run per set\n",
    "\"num_of_sets\" : 5, # Number of sets of pseudo-experiments to run\n",
    "} \n",
    "\n",
    "USE_RANDOM_MUS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Submissions\n",
    "By this point you should have a clone of the repo which contains `HiggsML_Dummy_Submission.zip` which you can submit to the Competition\n",
    "\n",
    "For more sample submissions please check `/HEP-Challenge/example_submissions/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from numpy.random import RandomState\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory is /Users/ihsanullah/Desktop/ParisSaclay/Fair Universe/HEP-Challenge\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root_dir = os.getcwd()\n",
    "print(\"Root directory is\", root_dir)\n",
    "\n",
    "input_dir = os.path.join(root_dir, \"input_data\")\n",
    "output_dir = os.path.join(root_dir, \"sample_result_submission\")\n",
    "submission_dir = os.path.join(root_dir, \"sample_code_submission\")\n",
    "program_dir = os.path.join(root_dir, \"ingestion_program\")\n",
    "score_dir = os.path.join(root_dir, \"scoring_program\")\n",
    "    \n",
    "test_settings = TEST_SETTINGS.copy()\n",
    "\n",
    "if USE_RANDOM_MUS:\n",
    "    test_settings[ \"ground_truth_mus\"] = (np.random.uniform(0.1, 3, test_settings[\"num_of_sets\"])).tolist()\n",
    "    \n",
    "    random_settings_file = os.path.join(output_dir, \"random_mu.json\")\n",
    "    with open(random_settings_file, \"w\") as f:\n",
    "        json.dump(test_settings, f)\n",
    "else:\n",
    "    test_settings_file = os.path.join(input_dir, \"test\", \"settings\", \"data.json\")\n",
    "    with open(test_settings_file) as f:\n",
    "        test_settings = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Add directories to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.append(program_dir)\n",
    "path.append(submission_dir)\n",
    "path.append(score_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Internal imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import *\n",
    "from systematics import systematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Import Submission Model\n",
    "***\n",
    "We import a class named `Model` from the submission file (`model.py`). This `Model` class has the following methods:\n",
    "- `init`: receives train set and systematics class as input\n",
    "- `fit`: can be used for training\n",
    "- `predict`: receives one test set and outputs a dictionary with the following keys\n",
    "    - `mu_hat` : predicted mu $\\hat{\\mu}$\n",
    "    - `delta_mu_hat`: $\\Delta{\\hat{\\mu}}$ bound for $\\mu$\n",
    "    - `p16`: 16th percentile\n",
    "    - `p84`: 84th percentile\n",
    "\n",
    "In this example code, the `Model` class implements an XGBoost model which is trained to predict both the TES and the class label. You can find the code in `HEP-Challenge/sample_code_submission/model.py`. You can modify it the way you want, keeping the required class structure and functions there. More instructions are given inside the `model.py` file. If running in Collab, click the folder icon in the left sidebar to open the file browser.\n",
    "\n",
    "### ⚠️ Note:\n",
    "In real setting i.e. the challenge itself, the submitted model is initialized once with train set and systematics class and the `predict` is called multiple times, each time with a different test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from datasets import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data\n",
    "***\n",
    "\n",
    "### ⚠️ Note:\n",
    "The data used here is a small sample data is for demonstration only to get a view of what the data looks like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`USE_PUBLIC_DATASET` determines whether to use a public dataset provided for the participants or use a small sample datafor quick execution of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PUBLIC_DATASET = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`USE_PUBLIC_DATASET` determines whether to use a public dataset provided for the participants or use a small subset of the data for quick execution of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] public_data directory not found\n",
      "[*] Extracting public_data.zip\n"
     ]
    }
   ],
   "source": [
    "if USE_PUBLIC_DATASET:\n",
    "    from datasets import Neurips2024_public_dataset as public_dataset\n",
    "    data = public_dataset()\n",
    "else:\n",
    "    data = Data(input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function loads the downloaded data in the public_data folder or downloads the data from codabench using `wget` in the absence of the downloaded data, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading Train data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 587494 entries, 0 to 587493\n",
      "Columns: 28 entries, PRI_lep_pt to DER_lep_eta_centrality\n",
      "dtypes: float32(23), float64(5)\n",
      "memory usage: 74.0 MB\n",
      "None\n",
      "[*] Train data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# load train set\n",
    "data.load_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading Test data\n",
      "[*] Test data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# load test sets\n",
    "data.load_test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Visualize\n",
    "***\n",
    "- Visualize Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_visualize = Dataset_visualise(\n",
    "    data_set=data.get_train_set(),\n",
    "    columns=[\n",
    "        \"PRI_jet_leading_pt\",\n",
    "        \"PRI_met\",\n",
    "        \"DER_mass_vis\",\n",
    "        \"DER_mass_jet_jet\",\n",
    "        \"DER_sum_pt\",\n",
    "    ],\n",
    "    name=\"Train Set\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data summary\n",
    "train_visualize.examine_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data summary\n",
    "train_visualize.histogram_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_visualize.stacked_histogram(\"DER_deltar_had_lep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data summary\n",
    "train_visualize.pair_plots(sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syst_train_data = data.get_syst_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plots of train set with systematics\n",
    "train_visualize.pair_plots_syst(syst_train_data[\"data\"], sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstraped_data = data.generate_psuedo_exp_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data summary\n",
    "train_visualize.examine_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum of weights = \", (bootstraped_data[\"weights\"]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Program\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ingestion import Ingestion\n",
    "\n",
    "ingestion = Ingestion(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize submission\n",
    "ingestion.init_submission(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit submission\n",
    "ingestion.fit_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load test set\n",
    "data.load_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict submission\n",
    "ingestion.predict_submission(test_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion.compute_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save result\n",
    "ingestion.save_result(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Score\n",
    "***\n",
    "1. Compute Scores\n",
    "2. Visualize Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scatter(ingestion_result_dict, ground_truth_mus):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for key in ingestion_result_dict.keys():\n",
    "        ingestion_result = ingestion_result_dict[key]\n",
    "        mu_hat = np.mean(ingestion_result[\"mu_hats\"])\n",
    "        mu = ground_truth_mus[key]\n",
    "        plt.scatter(mu, mu_hat, c='b', marker='o')\n",
    "    \n",
    "    plt.xlabel('Ground Truth $\\mu$')\n",
    "    plt.ylabel('Predicted $\\mu$ (averaged for 100 test sets)')\n",
    "    plt.title('Ground Truth vs. Predicted $\\mu$ Values')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_coverage(ingestion_result_dict, ground_truth_mus):\n",
    "\n",
    "    for key in ingestion_result_dict.keys():\n",
    "        plt.figure( figsize=(5, 5))\n",
    "\n",
    "        ingestion_result = ingestion_result_dict[key]\n",
    "        mu = ground_truth_mus[key]\n",
    "        mu_hats = np.mean(ingestion_result[\"mu_hats\"])\n",
    "        p16s = ingestion_result[\"p16\"]\n",
    "        p84s = ingestion_result[\"p84\"]\n",
    "        \n",
    "        # plot horizontal lines from p16 to p84\n",
    "        for i, (p16, p84) in enumerate(zip(p16s, p84s)):\n",
    "            plt.hlines(y=i, xmin=p16, xmax=p84, colors='b', label='p16-p84')\n",
    "\n",
    "        plt.vlines(x=mu_hats, ymin=0, ymax=len(p16s), colors='r', linestyles='dashed', label='Predicted $\\mu$')\n",
    "        plt.vlines(x=mu, ymin=0, ymax=len(p16s), colors='g', linestyles='dashed', label='Ground Truth $\\mu$')\n",
    "        plt.xlabel('mu')\n",
    "        plt.ylabel('psuedo-experiments')\n",
    "        plt.title(f'mu distribution - Set_{key}')\n",
    "        plt.legend()\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score import Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Score\n",
    "score = Scoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.load_ingestion_results(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Score\n",
    "score.compute_scores(test_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scatter plot of ground truth mu and predicted mu\n",
    "visualize_scatter(ingestion_result_dict=ingestion.results_dict, \n",
    "                  ground_truth_mus=test_settings[\"ground_truth_mus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coverage\n",
    "visualize_coverage(ingestion_result_dict=ingestion.results_dict, \n",
    "                  ground_truth_mus=test_settings[\"ground_truth_mus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Submission\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare the submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from data_io import zipdir\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "code_submission = 'HiggsML-code_submission_' + the_date + '.zip'\n",
    "zipdir(code_submission, submission_dir)\n",
    "print(\"Submit : \" + code_submission + \" to the competition\")\n",
    "print(\"You can find the zip file in `HEP-Challenge/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9e001b0608738f9411416229c98988c04b997dc526fb61c5e4e084e768e3249"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
